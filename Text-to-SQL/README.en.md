# 📊 Project Report: Developing a Korean Text-to-SQL Model Based on LLaMA 3.1

## 1. Project Overview
This project's goal was to develop a Korean-specialized Text-to-SQL model that converts natural language queries in Korean into SQL queries. We aimed to address the lack of Korean datasets and specialized models in the predominantly English-centric Text-to-SQL research landscape. Our objective was to contribute to the **Korean** AI ecosystem by building an open-source model and dataset that anyone can easily utilize.

To achieve this, we translated and refined the standard benchmark dataset, 'Spider,' into high-quality Korean. We then performed efficient fine-tuning using 4-bit quantization and the LoRA technique. The final model, dataset, and a real-time demo have all been publicly released through Hugging Face.

## 2. Key Achievements Summary

| Item | Description |
| :--- | :--- |
| **🗂️ Dataset Construction** | Built a high-quality Korean training dataset through translation error analysis and quality improvement. |
| **🤖 Model Development** | Developed a model specialized for the Text-to-SQL task using the Korean dataset. |
| **🚀 Performance Improvement** | Achieved **Exact Match (EM) of 42.65%** and **Execution Accuracy (EX) of 65.47%** on the Spider validation set. |
| **🌐 Public Release** | Released the model and Korean dataset on the Hugging Face Hub for public access. |
| **▶️ Live Demo Deployment** | Deployed an interactive demo application on Hugging Face Spaces for live testing of the model. |

## 3. Performance Evaluation Results

### 3.1. Explanation of Evaluation Metrics: Exact Match (EM) vs. Execution Accuracy (EX)
The performance of Text-to-SQL models is primarily evaluated using two metrics:

* **Exact Match (EM)**: A strict metric that assesses whether the SQL query generated by the model is a perfect string match with the ground-truth SQL query. (*Note: For a fair comparison, all queries in this evaluation underwent a normalization process.*)
* **Execution Accuracy (EX)**: Assesses whether the query generated by the model produces the same result as the ground-truth query when executed on a database.

**Why Execution Accuracy (EX) is More Important:**
In **practical applications**, Execution Accuracy is a better indicator of a model's performance. This is because a semantically identical SQL query can be written in multiple ways. For instance, using different table aliases (`... AS T1` vs. `... AS T2`) or changing the order of tables in a `JOIN` clause can produce the same execution result. These minor syntactic differences lower the string-based EM score but do not affect the actual query result.

> **Note**: If the order of columns in the `SELECT` clause changes, the structure of the resulting table is altered, causing it to be marked as incorrect for both EM and EX.

### 3.2. Final Performance
The following results were obtained by measuring the performance of the fine-tuned model on the Spider validation set (1,034 examples).

| Metric | Performance | Remarks |
| :--- | :--- | :--- |
| **Exact Match (EM)** | **42.65%** (441/1034) | The SQL query is an exact match after normalization. |
| **Execution Accuracy (EX)**| **65.47%** (677/1034) | The SQL query execution result is a match. |

### 3.3. Comparative Analysis with the Original Model
To demonstrate the effectiveness of fine-tuning, we conducted a comprehensive comparison between the performance of the original LLaMA 3.1 model and our fine-tuned model.

| Model | Language | Exact Match (EM) | Execution Accuracy (EX) | Remarks |
| :--- | :--- | :--- | :--- | :--- |
| **Original LLaMA 3.1** | English | 2.22% | 7.83% | On the original English dataset |
| **Original LLaMA 3.1** | Korean | 0.39% | 0.87% | Project Baseline |
| **Fine-tuned Model** | **Korean** | **42.65%** | **65.47%** | **Final Result** |

**Analysis:**

1.  **Dramatic Improvement in Korean Performance**: After fine-tuning, the model's ability to handle Korean queries showed a ~75-fold increase in Execution Accuracy (EX), from **0.87% to 65.47%**. This signifies that fine-tuning with a Korean-specialized dataset **dramatically improved** the model's understanding of the language for this specific task.
2.  **The Effect of Task-Specific Tuning**: **The key takeaway is** the effectiveness of task-specific tuning. The fine-tuned model's Execution Accuracy on Korean queries (65.47%) far surpassed the original model's performance on English queries (7.83%). This suggests that the process of fine-tuning with highly specialized data fundamentally **boosted** the model's problem-solving capabilities, **transcending** simple knowledge transfer to another language.

### 3.4. SQL Normalization for Exact Match (EM) Evaluation
Since Exact Match involves a direct string comparison, it can fail due to differences in whitespace, capitalization, or quote types, which are irrelevant to the SQL's meaning. For a fair comparison, both the model-generated queries and the ground-truth queries underwent the following normalization process before being compared.

```python
def normalize_sql(sql):
    import re
    if not isinstance(sql, str):
        return sql
    # 1. Extract values in quotes and store them temporarily
    quoted_values = []
    def replace_quotes(match):
        content = match.group(1) or match.group(2)
        placeholder = f"__QUOTED_VALUE_{len(quoted_values)}__"
        quoted_values.append(content)
        return f"'{placeholder}'"
    pattern = r"'([^']*)'|\"([^\"]*)\""
    sql_with_placeholders = re.sub(pattern, replace_quotes, sql)

    # 2. Normalize the rest of the SQL (identifiers, keywords, etc.)
    normalized_sql = sql_with_placeholders.lower().strip()
    normalized_sql = re.sub(r'\s+', ' ', normalized_sql)
    normalized_sql = re.sub(r'\s*,\s*', ', ', normalized_sql)
    normalized_sql = re.sub(r';+\s*$', '', normalized_sql)

    comparison_ops = ['!=', '<>', '>=', '<=', '=', '>', '<']
    for op in comparison_ops:
        pattern = r'\s*' + re.escape(op) + r'\s*'
        normalized_sql = re.sub(pattern, f' {op} ', normalized_sql)

    arithmetic_ops = ['+', '-', '*', '/']
    for op in arithmetic_ops:
        pattern = r'\s*' + re.escape(op) + r'\s*'
        normalized_sql = re.sub(pattern, f' {op} ', normalized_sql)

    keyword_ops = ['and', 'or', 'not', 'in', 'like', 'between']
    for op in keyword_ops:
        pattern = r'\s+' + op + r'\s+'
        normalized_sql = re.sub(pattern, f' {op} ', normalized_sql, flags=re.IGNORECASE)

    normalized_sql = re.sub(r'\s+', ' ', normalized_sql)

    # 3. Re-insert the original quoted values
    for i, value in enumerate(quoted_values):
        placeholder = f"__quoted_value_{i}__"
        normalized_sql = normalized_sql.replace(f"'{placeholder}'", f"'{value}'")

    return normalized_sql.strip()
```

## 4. Model and Dataset Details

### 4.1. Model Information
-   **Hugging Face Model**: [huggingface-KREW/Llama-3.1-8B-Spider-SQL-Ko](https://huggingface.co/huggingface-KREW/Llama-3.1-8B-Spider-SQL-Ko)
-   **Hugging Face Dataset**: [huggingface-KREW/spider-ko](https://huggingface.co/datasets/huggingface-KREW/spider-ko) (Translated and reviewed Korean version of the Spider dataset)
-   **Hugging Face Spaces Demo**: [Hugging Face Spaces](https://huggingface.co/spaces/huggingface-KREW/Llama-3.1-8B-Spider-SQL-Ko)
-   **Base Model**: `unsloth/Meta-Llama-3.1-8B-Instruct` (with 4-bit quantization)

### 4.2. Dataset Construction and Review
During the initial translation process, we identified issues where the LLM added unnecessary explanations or incorrectly translated comparative expressions, subject-predicate agreement, and domain-specific terminology. To resolve this, we improved the translation prompts and significantly enhanced data quality through manual review and error analysis. (See Appendix for details).

### 4.3. Fine-tuning Details
-   **GPU**: NVIDIA Tesla T4 (16GB)
-   **Library**: Unsloth
-   **VRAM Usage**: Up to 7.6GB
-   **Training Time**: Approx. 4 hours
-   **Training Code**: [Google Colab](https://drive.google.com/file/d/1tAlGr7t8r60j2jxafyhP0MGQU7s0j1XH/view?usp=sharing)
-   **Key Hyperparameters**:
    ```python
    training_args = {
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "learning_rate": 5e-4,
        "num_train_epochs": 1,
        "optimizer": "adamw_8bit",
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.05
    }

    lora_config = {
        "r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                           "gate_proj", "up_proj", "down_proj"]
    }
    ```

## 5. Model Usage and Demo

### 5.1. How to Use the Hugging Face Demo
1.  **Access the [Hugging Face Space](https://huggingface.co/spaces/huggingface-KREW/Llama-3.1-8B-Spider-SQL-Ko)**
2.  Enter the database schema information (table names, column names, etc.).
3.  Write and submit your question in Korean.
4.  Review the generated SQL query.

### 5.2. How to Use in a Local Environment

```python
# 1. Install required libraries
!pip install "unsloth[colab-new]" -q
!pip install --no-deps trl peft accelerate bitsandbytes

# 2. Load the model and tokenizer
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="huggingface-KREW/Llama-3.1-8B-Spider-SQL-Ko",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# 3. Define and run the SQL generation function
def generate_sql(question, schema_info):
    # Note: The model expects a Korean question.
    prompt = f"""Based on the following database schema, generate a SQL query for the given question.

### Database Schema:
{schema_info}

### Question: {question}

### SQL Query:"""

    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")

    outputs = model.generate(inputs, max_new_tokens=150, temperature=0.1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response.split("### SQL Query:")[-1].strip()

# 4. Example usage
schema = """Table: singer
Columns: singer_id, name, country, age"""

# The question must be in Korean for this model.
korean_question = "가수는 몇 명이 있나요?" # "How many singers are there?"

query = generate_sql(korean_question, schema)
print(query)
# Output: SELECT count(*) FROM singer
```

## 6. Limitations and Future Work

### 6.1. Current Limitations
-   **English Schema Restriction**: Table and column names must currently be in English.
-   **Domain Constraints**: The model is optimized for the domains included in the Spider dataset.
-   **Complex Query Limitations**: Accuracy may decrease for very complex nested queries.

### 6.2. Future Improvements
-   **Dataset Quality Enhancement**: We plan to provide an additional version of the dataset with consistent naming conventions (e.g., for case sensitivity, abbreviations) to improve developer convenience.
-   **Data Augmentation**: We will add more data with diverse question styles and complexities to improve model robustness.
-   **Model Scaling**: We will attempt to improve accuracy by experimenting with larger models (e.g., 13B or larger).

---
## Appendix: Key Error Types Found During Dataset Translation and Review

To improve the quality of the translated data, we conducted a manual review and identified the following major error types:

| Error Type | Details | Correction |
| :--- | :--- | :--- |
| 1. **Time/Number Comparison Errors** | `more than` was translated to a Korean term meaning "greater than or equal to." | Corrected to use precise terms that exclude the boundary value, like "greater than" or "less than." |
| 2. **Awkward Korean Phrasing** | Use of unnatural subjects, redundant modifiers, or awkward vocabulary. | Revised to be more natural in a Korean context by omitting or rephrasing words. |
| 3. **Subject-Predicate Mismatch** | A question asking for a count or age used an incorrect interrogative pronoun ("what" instead of "how many"). | Corrected to use the appropriate predicate for the subject. |
| 4. **Domain Terminology Mistranslation** | `production time` was translated literally instead of as "production date." | Corrected to use appropriate domain-specific terms like "production date" or "acceleration performance." |
| 5. **Omission or Alteration of Meaning** | `average` was translated simply as "average," omitting part of the intended meaning like "average capacity." | Revised to accurately reflect the original intent by adding the missing information. |
