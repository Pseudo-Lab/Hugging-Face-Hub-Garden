#!/bin/bash

# ê³µí†µ ì„¤ì •
DATA_PATH="huggingface-KREW/KoCulture-Dialogues-v2"
MAX_SEQ_LENGTH=512
BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=64
LEARNING_RATE=6e-5
NUM_EPOCHS=3

# í•™ìŠµí•  ëª¨ë¸ ëª©ë¡
# MODELS="kakaocorp/kanana-1.5-8b-instruct-2505 LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
# MODELS="LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
MODELS="unsloth/Qwen3-8B"

# ê° ëª¨ë¸ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ ì‹¤í–‰
for MODEL_NAME in $MODELS; do
    echo "========================================="
    echo "Starting training for: $MODEL_NAME"
    echo "========================================="
    
    # í˜„ì¬ ì‹œê°„ ê¸°ë¡
    START_TIME=$(date)
    echo "Start time: $START_TIME"
    
    
    # í•™ìŠµ ì‹¤í–‰
    CUDA_VISIBLE_DEVICES=1 uv run train_transformers_full_fine_tuning.py \
        --model_name "$MODEL_NAME" \
        --data_path "$DATA_PATH" \
        --max_seq_length $MAX_SEQ_LENGTH \
        --batch_size $BATCH_SIZE \
        --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
        --learning_rate $LEARNING_RATE \
        --num_epochs $NUM_EPOCHS \
        --private
    
    # í•™ìŠµ ê²°ê³¼ í™•ì¸
    if [ $? -eq 0 ]; then
        echo "âœ… Training completed successfully for: $MODEL_NAME"
    else
        echo "âŒ Training failed for: $MODEL_NAME"
        echo "Continuing with next model..."
    fi
    
    # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    END_TIME=$(date)
    echo "End time: $END_TIME"
    echo "========================================="
    echo ""
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ì ì‹œ ëŒ€ê¸°
    sleep 10
done

echo "ğŸ‰ All model training completed!"